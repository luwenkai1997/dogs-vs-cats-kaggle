{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dogs vs Cats",
   "id": "8350c3b29f0e44a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 1: Setting up the Environment",
   "id": "10783ae453d4a4d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T06:34:42.480520Z",
     "start_time": "2024-11-13T06:34:06.095852Z"
    }
   },
   "cell_type": "code",
   "source": "# !pip install torch torchvision matplotlib",
   "id": "3406284d9561ac7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\r\n",
      "  Downloading torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl (150.8 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m150.8/150.8 MB\u001B[0m \u001B[31m5.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting torchvision\r\n",
      "  Downloading torchvision-0.17.2-cp311-cp311-macosx_10_13_x86_64.whl (1.7 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m10.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: matplotlib in /Users/luwenkai/anaconda3/lib/python3.11/site-packages (3.7.1)\r\n",
      "Requirement already satisfied: filelock in /Users/luwenkai/anaconda3/lib/python3.11/site-packages (from torch) (3.9.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/luwenkai/anaconda3/lib/python3.11/site-packages (from torch) (4.12.1)\r\n",
      "Requirement already satisfied: sympy in /Users/luwenkai/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\r\n",
      "Requirement already satisfied: networkx in /Users/luwenkai/anaconda3/lib/python3.11/site-packages (from torch) (2.8.4)\r\n",
      "Requirement already satisfied: jinja2 in /Users/luwenkai/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /Users/luwenkai/anaconda3/lib/python3.11/site-packages (from torch) (2023.3.0)\r\n",
      "Requirement already satisfied: numpy in /Users/luwenkai/anaconda3/lib/python3.11/site-packages (from torchvision) (1.24.3)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/luwenkai/anaconda3/lib/python3.11/site-packages (from torchvision) (10.3.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/luwenkai/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.0.5)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/luwenkai/anaconda3/lib/python3.11/site-packages (from matplotlib) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/luwenkai/anaconda3/lib/python3.11/site-packages (from matplotlib) (4.25.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/luwenkai/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.4.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/luwenkai/anaconda3/lib/python3.11/site-packages (from matplotlib) (23.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/luwenkai/anaconda3/lib/python3.11/site-packages (from matplotlib) (3.0.9)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/luwenkai/anaconda3/lib/python3.11/site-packages (from matplotlib) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/luwenkai/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/luwenkai/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/luwenkai/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.2.1)\r\n",
      "Installing collected packages: torch, torchvision\r\n",
      "Successfully installed torch-2.2.2 torchvision-0.17.2\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 2: Import Necessary Libraries",
   "id": "c0f52baa07bd2052"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T06:51:03.821577Z",
     "start_time": "2024-11-13T06:51:00.104890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "3b3804ab1c9a2bd9",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 3: Data Preparation",
   "id": "c1f0d439a192f5d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3.1 Define Data Transformations\n",
    "Use transformations to resize images, normalize pixel values, and apply data augmentation for the training set to make the model more robust."
   ],
   "id": "bc432ce7dae89468"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T06:51:06.646500Z",
     "start_time": "2024-11-13T06:51:06.640523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "}"
   ],
   "id": "7c8bbc074a8ec55c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.2 Load Data",
   "id": "6529e853c99b05c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T06:51:07.815548Z",
     "start_time": "2024-11-13T06:51:07.735966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_dir = \"data/train\"\n",
    "dataset = datasets.ImageFolder(data_dir, transform=data_transforms['train'])\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ],
   "id": "fb93affc725ea842",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3.3 Create DataLoaders\n",
    "DataLoaders feed data to the model in batches, which is efficient for training."
   ],
   "id": "e8cb3125c076780f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T06:51:09.083821Z",
     "start_time": "2024-11-13T06:51:09.078779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "cdcf883a3f4d1d67",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 4: Define the Model\n",
    "We can use a pre-trained model (like ResNet or VGG) and fine-tune it for our classification task. For this project I load a ResNet18 model with pre-trained weights, replaces the final layer to match the number of classes (2: cats and dogs), and sets up the modified network for training."
   ],
   "id": "7068143198a30215"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T06:52:23.507294Z",
     "start_time": "2024-11-13T06:52:23.319359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)  # Change the output layer for binary classification"
   ],
   "id": "c6e82be2bd3a19d7",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 5: Set Up Training Components",
   "id": "a5c000f002e51a04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 5.1 Define Loss Function and Optimizer",
   "id": "f16c25ac0bfc450b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T06:53:10.887627Z",
     "start_time": "2024-11-13T06:53:10.881169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "4da4b3c9064f8901",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 5.2 Move Model to GPU if Available",
   "id": "a525feca65952737"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T06:53:33.043110Z",
     "start_time": "2024-11-13T06:53:33.035737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ],
   "id": "6c8fad161bd3dc54",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 6: Train the Model\n",
    "Define the training loop to go through several epochs, adjusting weights to minimize the loss. This code trains the model over num_epochs epochs and prints out the training loss and validation accuracy for each epoch."
   ],
   "id": "9f216b4de02776e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T07:02:32.342650Z",
     "start_time": "2024-11-13T06:55:18.553137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += torch.sum(preds == labels).item()\n",
    "\n",
    "    val_accuracy = correct / len(val_loader.dataset)\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n"
   ],
   "id": "526921d39c969856",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 13\u001B[0m\n\u001B[1;32m     11\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(inputs)\n\u001B[1;32m     12\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[0;32m---> 13\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     14\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     16\u001B[0m running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m*\u001B[39m inputs\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    521\u001B[0m     )\n\u001B[0;32m--> 522\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[1;32m    523\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[1;32m    524\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 266\u001B[0m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    267\u001B[0m     tensors,\n\u001B[1;32m    268\u001B[0m     grad_tensors_,\n\u001B[1;32m    269\u001B[0m     retain_graph,\n\u001B[1;32m    270\u001B[0m     create_graph,\n\u001B[1;32m    271\u001B[0m     inputs,\n\u001B[1;32m    272\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    273\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    274\u001B[0m )\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 7: Evaluate the Model\n",
    "To evaluate your model on the validation set, calculate accuracy or other metrics."
   ],
   "id": "6127c2102a49f630"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += torch.sum(preds == labels).item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Validation Accuracy: {correct / total:.4f}\")"
   ],
   "id": "287ca3152f5819af"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 8: Save the Model\n",
    "Once the model is trained and evaluated, you can save it for future use."
   ],
   "id": "a5cdd0384235577a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.save(model.state_dict(), \"cats_vs_dogs_model.pth\")",
   "id": "581c5315dbe5b9ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 9: Test with New Images\n",
    "After training, you can test your model with new images."
   ],
   "id": "6ad14fde72f42c62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from PIL import Image\n",
    "\n",
    "def predict_image(image_path, model, transform):\n",
    "    image = Image.open(image_path)\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, pred = torch.max(output, 1)\n",
    "\n",
    "    return 'Dog' if pred.item() == 1 else 'Cat'\n",
    "\n",
    "# Example usage\n",
    "print(predict_image(\"data/test1/1.jpg\", model, data_transforms['val']))\n"
   ],
   "id": "9f7dafa04578c957"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 10: Visualize Results\n",
    "Finally, visualize some predictions to see how well your model is performing."
   ],
   "id": "aebc74c28962bc13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def show_predictions(images, labels, preds):\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=(12, 4))\n",
    "    for i, (img, lbl, pred) in enumerate(zip(images, labels, preds)):\n",
    "        ax = axes[i]\n",
    "        ax.imshow(img.permute(1, 2, 0).cpu().numpy())\n",
    "        ax.set_title(f\"True: {'Dog' if lbl==1 else 'Cat'}, Pred: {'Dog' if pred==1 else 'Cat'}\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "# Get a batch of validation images and show predictions\n",
    "inputs, labels = next(iter(val_loader))\n",
    "inputs, labels = inputs[:5].to(device), labels[:5]\n",
    "outputs = model(inputs)\n",
    "_, preds = torch.max(outputs, 1)\n",
    "show_predictions(inputs.cpu(), labels, preds.cpu())\n"
   ],
   "id": "6091972f6cc44465"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
